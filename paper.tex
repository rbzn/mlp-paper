\documentclass[runningheads]{llncs}


\usepackage[utf8]{inputenc}
\usepackage[numbers, comma, sort]{natbib}
\bibliographystyle{apalike}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{float}


\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{arrows,backgrounds,positioning}


\usepackage{url}
\urldef{\mailsa}\path|rob@clabs.cc, schubotz@tu-berlin.de|
\newcommand{\todaydate}{\leadingzero{\day}.\leadingzero{\month}.\the\year}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter

\title{Mathematical Language Processing \\ Project}
\titlerunning{MLP}

\author{Robert Pagel \and Moritz Schubotz}
\authorrunning{Pagel, Schubotz}

\institute{Berlin Institute of Technology, Database Systems and Information Management Group,\\
Einsteinufer 17, 10587 Berlin, Germany\\
\mailsa\\
\url{http://www.dima.tu-berlin.de/}}


\maketitle


\begin{abstract}
This paper presents automated definition discovery for
variables occurring in mathematical formulae. Ordinary natural language
processing approaches are trained based on large, manually annotated news
corpora. They fall short in reasonable quality of definition discovery for
scientific texts. In the \emph{Mathematical Language Processing} project, we
discover definitions
%we did not discover any relations  a relation would be something like "let
%<subj> be a <object>", or "we call <subj> <obj>", or "<subject> is <object>"
%or just "<subj> <obj>"
for identifiers in formulae from the surrounding text by statistical methods.
%with a machine learning approach for classification. This was also not done
%in this paper We extract the identifiers from the formulae and track co-
%occurrences of those in the surrounding text.
The evaluation of our prototypical system, applied on the Wikipedia text
corpus shows that our approach augments the user experience for articles with
mathematical formula substantially. While hovering the identifiers occurring
in the equation a pop-up with the most probable explanations for the
identifiers occurs. The displayed definition information provides a good match
to the actual meaning of the identifiers.
\keywords{machine learning, text mining, parallel computing}
\end{abstract}


\section{Introduction}

Mathematical formulae are a viable source of information for a wide range of
scientists. Often those formulas contain variables that are unknown or at
least ambiguous to the reader. Therefore, one needs to study the surrounding
text to find the relevant definition.

An automatic information retrieval system can be used to reduce the readers
effort to understand a formula. Especially students and scientists of other
disciplines would profit from such a system.

Unfortunately, no one has built a labeled text corpus that annotates variables
and their definition. Instead of labeling large amounts of text by hand, we
investigate a potential way of automatically labeling large corpora with a set
of statistical assumptions.

We chose the Wikipedia text corpus as a target because of two facts. First,
the mark-up for editing formulas is essentially a subset of \TeX. Therefore,
it enables us to collect all variables within the set of formulas in a
document. Second, we can leverage the semantic information between mark-up and
word tokens, e.g., hyperlinks.

The English Wikipedia contains roughly four million articles. Even if we only
pick articles containing \texttt{<math/>} mark-up, our processor still needs
to compute tens of thousands of articles. Especially when using a maximum
entropy \emph{POS} tagger \cite{Rathna96}, like the one in Stanford's NLP
framework, one can make use of a parallel processing system to speed up
computation.

We implement  show how the proposed strategy can be implemented in the PACT
programming model \cite{Alexandrov2010}.


\paragraph{Related Work}
\citeauthor{Quoc2010}\cite{Quoc2010} proposed an approach for
relating whole formulas to sentences and their describing paragraphs.
\citeauthor{Yokoi}\cite{Yokoi} trained a emph{support vector machine} to extract
natural language descriptions for mathematical expressions.


\section{Statistical definition discovery}
We detect relations between identifiers and their description in two steps.
First, we extract the identifiers from the display-style formulae and
second we determine their description from the surrounding text.

Inspired by the \emph{Distant Supervision} approach \cite{Mintz2008},
we assume that the identifier and the definition co-occur in the same sentence.
Furthermore, we restrict ourselves to identifiers that co-occur with a noun
phrase in one sentence in the document at least.

For each description candidate we use the weighted sum
\begin{equation} \label{eq:rating}
	R(n,\Delta,t,d)=\frac{\alpha{R}_{\sigma_\mathrm d}(\Delta)
		+\beta{R}_{\sigma_\mathrm s}(n)
		+\gamma\mathrm{tf}(t,d)}{\alpha+\beta+\gamma} \mapsto [0;1]
\end{equation}
that respects the distance $\Delta$ between identifier and definition term $t$, the distance $n$ between the display-style equation and th sentence that contains the term and the identifier and the term frequency $\mathrm{tf}(t,d)$ in the current document $d$ 
with a normal distributed distance measure $R_\sigma(\Delta)= \exp\left[-\frac{1}{2}\frac{\Delta^2-1}{\sigma^2}\right].$
Our model assumption is that the probability to find a relation at $\Delta=1$ like for example in \emph{the energy $E$ and the mass $m$} is maximal, i.e. $R_{\sigma_\mathrm d}(1)=1$.  In order to determine the full width half maximum of our distribution we evaluated some articles manually and found $R_{\sigma_\mathrm d}(1)\approx 2 R_{\sigma_\mathrm d}(5)$ and thus $\sigma_d=\sqrt\frac{5^2-1}{2\ln 2}$.

The probability to find a correct definition decays for 50 \% within three sentences. Consequently  $\sigma_\mathrm s=2\left({\ln 2}\right)^{-\frac{1}{2}}$.

The classic tf-idf \cite{Salton86} statistic reflects the importance of a term to a document. For our task the inverse document frequency (idf)  would assign high penalties to frequent which are valid definitions for identifiers like in the example `Energy` and `Einstein`. Consequently we used just the term frequency which still had an impact on the overall result that seemed to high in our manual experiments. Therefore, we use the tuning parameters $\gamma=0.75$ and $\alpha = \beta = 1$.


%The weighting parameters $\alpha$, $\beta$, and $\gamma$ in Equation
%\ref{eq:rating} are used to balance the contribution of all embedded
%statistics to the overall ranking. But these parameters need to be tuned
%carefully. Otherwise the results tend to be biased towards one parameter. In
%other words, if the distance measure $R_{D}$ dominates the other measures,
%more complex\footnote{in terms of words between both entities, participating
%in a relation} relations are less likely to be discovered.
%
%The \emph{term frequency} turns out to be the least robust statistic. Usually,
%the frequency of a specific term co-occurring with a variable name within one
%document is rather low. Thus, the derived ranking is relatively volatile. In
%our experiments, we chose $\gamma$ to be 0.75 ($\alpha = \beta = 1$) for a
%more robust ranking.
%to  E.g., `Hamiltonian' as well as `length' are both valid entities. Obviously, the number of documents containing one of these words, within a corpus like Wikipedia, differs a lot.
%
%Therefore, we only use the normalized term frequency. $t$ and $w$ are terms.
%$f(t,d)$ denotes the frequency of a term within a document $d$.
%
%\begin{equation}
%	\label{eq:tf}
%	\mathrm{tf}(t,d) = \frac{\mathrm{f}(t,d)}{\max\{\mathrm{f}(w,d):w \in d\}}
%\end{equation}
%

\section{Implementation}
The MLP processing system \cite{github} is basically a set of PACTs, where
data flows through. In contrast to other programming models like Map/Reduce,
PACTs can operate on multiple inputs. At some point of time, one needs an
ordered list of tokenized sentences and a list of identified variables, to
compute the ranking score. A \emph{CoGroup Contract} takes both lists and
computes the subsets, build upon distinct document keys. Finally, a
\emph{Reduce Contract} filters out results below a given threshold $\theta$.

\tikzset{actor/.style={
        rectangle,
        minimum size=6mm,
        very thick,
        draw=gray!50!black!50,
        top color=white,
        bottom color=gray!50!black!20
    },
    arrow/.style={
        -latex, thick, shorten <=2pt,shorten >=2pt
    }
}
\begin{figure}[H]
	\begin{tikzpicture}[node distance=5mm and 8mm]
		\node (Input) [align=center]{Wiki Dumps};
		\node (DocumentParser) [actor, right=of Input, align=center] {\emph{Map}\\\textbf{Parser}};
		\node (Candidates) [actor, right=of DocumentParser, align=center] {\emph{CoGroup}\\\textbf{Kernel}};
		\node (Sentence) [actor, above=of DocumentParser, align=center] {\emph{Map}\\\textbf{Tagger}};
		\node (Filter) [actor, right=of Candidates, align=center] {\emph{Reduce}\\\textbf{Filter}};
		\node (Output) [right=of Filter,align=center]{Raw\ Candidates};
		\draw[arrow] (Input)--(DocumentParser);
		\draw[arrow] (DocumentParser)--(Sentence);
		\draw[arrow] (DocumentParser)--(Candidates);
		\draw[arrow] (Sentence.east)--(Candidates.north);
		\draw[arrow] (Candidates)--(Filter);
		\draw[arrow] (Filter)--(Output);
	\end{tikzpicture}
\caption{Data flow of the PACT program}
\end{figure}

There are three computation heavy parts in the processor. First, the raw
Wikipedia document dumps need to be transformed into plain text. At the same
step, the formulas need to be evaluated and replaced by place-holder words. We
used the \emph{WikiText} framework of the Mylyn project with a custom builder
to transform the document into plain text. For analysing the \TeX \ mark-up in
the \texttt{<math/>} tags, \emph{SnuggleTeX} transforms the formulas into
their MathML representations, where variable names can be easily extracted.

The second computation heavy part is tokenization and tagging of the plain
text documents. We used the Stanford maximum entropy tagger with the \emph{WSJ
left3words} language model.

Finally, the rating for all nouns, contained in all candidate sentences, needs
to be calculated.


%\subsection{Experiments}
\section{Evaluation}
%\subsubsection{Variable Retrieval}
\label{vr}
Throughout our experiments we made some observations that had an impact on the
accuracy of retrieving the correct set of variables. First of all, people tend
to incorrectly use \TeX\ trying to create formulas. E.g.,
\texttt{\textbackslash text\{log\}} is more often used than the correct
operator \texttt{\textbackslash log}. Another problem is that sometimes people
use indices as a form of annotation, like $T_{before}$, $R_{A}$, and $R_{B}$.
These ambiguities lead to a increased number of false positives.

We took a very conservative approach and preprocessed all formulas.
\texttt{\textbackslash text\{\}} blocks along with plain subscriptions will be
removed before analysis. Moreover, we created a comprehensive blacklist to
improve the results further. Short words like `a' and `i', which are very
common, could be easily matched by our processor if a formula contains one of
those symbols. Additionally, we blacklist common mathematical operators,
constants, and functions.

Because of the fact, that we do not have any annotated test corpora,
evaluation has to be performed by hand. Therefore, we took a sample of 30
random documents and counted all matches. The resulting estimates for
\emph{recall} and \emph{precision} are 0.99 and 0.86 respectively.




\section{Conclusion}
Our experiments showed that combining a POS tagger with numerical statistics
about the text surface, can lead to quality results. However, this approach is
only applicable under certain conditions. In situations where both parts of a
relation are unknown, other methods, especially supervised ones, are used to
bootstrap a language model. Unfortunately, we still need a labelled test
corpus to measure the performance of a classifier, trained with our generated
data.

Apart from the mentioned weighting parameters, the ranking's quality is
influenced by two further parameters. These are the assumptions about the
parameter $c$. Balancing all these parameters requires extensive knowledge
about the underlying language. These parameters might be specific for every
type of relation. Therefore, this strategy depends heavily on human input and
many iterations of tuning.

To further improve the robustness of the \emph{term frequency} measure, one
might cluster sets of scientific documents, based on their specific field of
research.



%460 identifiers
\begingroup
\let\clearpage\relax
\bibliography{mlp-papers}
\endgroup

\end{document}
